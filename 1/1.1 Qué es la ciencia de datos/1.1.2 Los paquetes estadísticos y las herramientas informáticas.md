# Herramientas en la Ciencia de Datos

La *ciencia de datos* aún está en proceso de definición, lo que destaca la importancia de estandarizar el software y las herramientas. Comparativamente, el software sería equiparable a los pinceles y piquetas utilizados por los primeros arqueólogos en sus excavaciones. Sin embargo, no se debe enfocar excesivamente el esfuerzo en dominar todas las herramientas, ya que estas no definen la capacidad de alguien como *científico de datos*. Es el método científico, no las herramientas, lo que realmente caracteriza a un *científico de datos*.

Estas herramientas se pueden clasificar en tres categorías básicas: almacenamiento, depuración y análisis. Para almacenar datos, se pueden utilizar hojas de cálculo, bases de datos y bases de datos clave-valor. Entre las más utilizadas se encuentran Hadoop, Cassandra y PostgreSQL. La depuración facilita el trabajo con datos y se vale de editores de texto, herramientas de secuencia de comandos, así como lenguajes de programación como Python y SCALLOP. Por último, existen paquetes estadísticos que ayudan en el análisis de datos, como el paquete de código abierto R, SBSS y las bibliotecas de datos de Python. Estas herramientas permiten visualizar datos y crear gráficos y diagramas.

Es crucial entender las herramientas para almacenar datos. El concepto de los "desafíos del big data" resuena fuertemente, ya que la cantidad de información es tal que los sistemas de gestión en base de datos se ven abrumados. Aunque se asocie a menudo la ciencia de datos con el *big data*, esta disciplina emplea el método científico para estudiar los datos, no limitándose a trabajar exclusivamente con *big data*.

Uno de los programas de código abierto más utilizado actualmente es Hadoop, que utiliza un sistema de archivos distribuido para almacenar datos en servidores estándar, organizados en un clúster Hadoop. Este clúster permite la ejecución de aplicaciones y procesos sobre datos distribuidos en petabytes en cientos o miles de servidores. Entre los procesos comunes se encuentran MapReduce, para trabajos por lotes, y Apache Spark, capaz de procesar datos en tiempo real.

Una vez recolectados los datos, entra en juego la depuración, un proceso esencial. A menudo, la información recolectada no es directamente utilizable, por lo que es necesario aplicar limpieza y organización. Es común crear scripts o comandos para dividir datos en categorías, como separar tuits en texto o imágenes. La depuración es una tarea que puede repetirse frecuentemente, lo que motiva la creación de aplicaciones con Python para automatizar estos procesos.

El análisis de datos, realizado principalmente con R o Python, consume una parte significativa del tiempo del *científico de datos*. R, un lenguaje de programación estadística, facilita la creación de conexiones y correlaciones entre datos, además de ofrecer herramientas de visualización incorporadas para presentar información de manera efectiva.

Estas herramientas son solo algunas de las más populares, pero en equipos de *data science* se mencionarán otras. La inversión en herramientas de recolección, depuración y análisis de datos continúa creciendo para satisfacer la demanda en constante expansión. Sin embargo, es fundamental priorizar el análisis sobre la adquisición de nuevas herramientas, ya que estas solo son un medio para adquirir conocimiento. El gasto en nuevo software debe ser limitado en pos de este enfoque analítico. 
 